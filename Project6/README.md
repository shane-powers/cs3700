# High Level Approach
## Milestone
To start this assignment, we both reviewed the Raft pdf document. We found this document to be very informative, and were able to develop a basic understanding of the protocol from it. We attempted to design our program to follow the Raft consensus algorithm summary given in the pdf. We began by implementing a basic leader election process. We set an abitrary timeout length, and made replicas check for a timeout in the while loop. If a timeout occured, that replica would initiate an election and send RequestVote messages to all of the other replicas. The other replicas would then check if the term in the RequestVote message was greater than or equal to the replicas current term. If that was the case, the replica then would reply with a vote. If the current candidate received a vote from the majority of the other replicas, it would make itself leader. After being elected, the leader sends heartbeat messages in the form of an AppendEntries with an empty entries field. This just serves to let the other replicas know that they are followers. When a replica that is not a leader receives a command from a client, it redirects the message to the leader. When a leader receives a put command, it puts the key value into storage, and sends out an AppendEntries message with the key value pair that was just entered into the log.
## To Completion
To build on from our work done in the milestone, there was actually very little more we needed to implement. We found our election and timeouts logic to be slightly incorrect but this will be covered in the next section. All we had to implement otherwise was the successful handling of append entries from the other replicas who received the message. For the milestone, we did not have the replicas do anything with the append entries outside of let the leader know it was received. For completion, we set up a store of key value pairs in each replica, and upon receipt of an append entries message, it added the contents of the message to its key value store. We also added in a 'catchup' mechanism. If a replica found itself to be behind the leader, it would repy a fail to the leader and let the leader know the last index it received. The leader then communicates with that replica to bring them back to speed. 

# Challenges Faced
## Milestone
For the milestone, we overcame many overhead issues, such as understanding the starter code provided to us, taking a ton of time to understand the Raft consensus algorithm in general, as well as writing a program in python to match our understanding of the algorithm we are looking to implement. Many of the challenges we faced were quickly able to be overcome with the use of print statements to track the flow of data and information across and between our nodes. We had to figure out what the purpose of the log was, and how our nodes should communicate information between each other whether that be a request vote, append entry, or a response from the leader to one of those two messages. We also had to handle get and put messages correctly and it took us time to understand how redirections work, and it turned out it was painfully easy, just a reflection back to the client. When we figured that out, and ran our code with print statements on the khoury machine, everything came together and we just had to diagnose issues as we saw them at runtime.
## To Completion
The challenges faced after the milestone were errors we found in our work done for the milestone. We found that our timeouts were too long, so we shortedned them up by a factor of 10. From (1,3) seconds to (.2,1) seconds. We then fixed how frequently heartbeats were sent by the leader to be just below the fastest replica instead of any shorter than that. We implemented these fixes because we found that too many messages were being sent back and forth between the replicas and wanted to keep as much credit as possible for the performance tests. We also faced the challenge of having the replicas store the key value pairs in their own store. This itself was easy, but what was more difficult was determining when a replica was behind a leader and what to do about it, to make sure that the replicas were on pace/up to speed with the current leader. We were able to implement this by having the follower reply with a fail if it got an log index more than one greater than its current log index. It will reply with a failure back to the leader with what its last known index was, and the leader will reply personally to that one follower and bring them up to speed. 

# Testing Overview
## Milestone
To test, we primatrily relied on print statements. Our program is very saturated with print statements so we can track the state of every node, and what actions/events it is doing whether it is a follower, candidate or leader. This testing method required a intuitive understanding of the protocol and how it is supposed to function to make sure that the print statements we are receiving are in fact the events that we are expecting to happen, and in accordance with the Raft consensus algorithm.
## To Completion
While we continued to use the same method of testing as we used in the milestone, we began to test with more focus on the tests in the testing suite. Our method of testing became making a change, and running it through the tests and observing what would fail. We would then focus on that test and address the failure and see if we can fix that aspect of our code. Then moving on to future tests. This caused a little bit of back and forth, making changes that resulted in errors in other areas of our code, but worked out in the long run. Eventually we were able to run through the whole test suite with minimal errors and were happy with our code base ready for submission.